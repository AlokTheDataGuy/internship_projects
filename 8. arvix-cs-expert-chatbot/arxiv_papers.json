{
  "papers": [
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    },
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    },
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    },
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    },
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    },
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    },
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    },
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    },
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    },
    {
      "id": "2507.14137v1",
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
      "authors": [
        "Shashanka Venkataramanan",
        "Valentinos Pariza",
        "Mohammadreza Salehi",
        "Lukas Knobel",
        "Spyros Gidaris",
        "Elias Ramzi",
        "Andrei Bursuc",
        "Yuki M. Asano"
      ],
      "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14137v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:59:55+00:00"
    },
    {
      "id": "2507.14126v1",
      "title": "Toward Temporal Causal Representation Learning with Tensor Decomposition",
      "authors": [
        "Jianhong Chen",
        "Meng Zhao",
        "Mostafa Reisi Gahrooei",
        "Xubo Yue"
      ],
      "abstract": "Temporal causal representation learning is a powerful tool for uncovering\ncomplex patterns in observational studies, which are often represented as\nlow-dimensional time series. However, in many real-world applications, data are\nhigh-dimensional with varying input lengths and naturally take the form of\nirregular tensors. To analyze such data, irregular tensor decomposition is\ncritical for extracting meaningful clusters that capture essential information.\nIn this paper, we focus on modeling causal representation learning based on the\ntransformed information. First, we present a novel causal formulation for a set\nof latent clusters. We then propose CaRTeD, a joint learning framework that\nintegrates temporal causal representation learning with irregular tensor\ndecomposition. Notably, our framework provides a blueprint for downstream tasks\nusing the learned tensor factors, such as modeling latent structures and\nextracting causal information, and offers a more flexible regularization design\nto enhance tensor decomposition. Theoretically, we show that our algorithm\nconverges to a stationary point. More importantly, our results fill the gap in\ntheoretical guarantees for the convergence of state-of-the-art irregular tensor\ndecomposition. Experimental results on synthetic and real-world electronic\nhealth record (EHR) datasets (MIMIC-III), with extensive benchmarks from both\nphenotyping and network recovery perspectives, demonstrate that our proposed\nmethod outperforms state-of-the-art techniques and enhances the explainability\nof causal representations.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "published": "2025-07-18T17:55:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14126v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:55:42+00:00"
    },
    {
      "id": "2507.14121v1",
      "title": "Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective",
      "authors": [
        "Pankaj Yadav",
        "Vivek Vijay"
      ],
      "abstract": "Kolmogorov Arnold Networks (KANs) are recent architectural advancement in\nneural computation that offer a mathematically grounded alternative to standard\nneural networks. This study presents an empirical evaluation of KANs in context\nof class imbalanced classification, using ten benchmark datasets. We observe\nthat KANs can inherently perform well on raw imbalanced data more effectively\nthan Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,\nconventional imbalance strategies fundamentally conflict with KANs mathematical\nstructure as resampling and focal loss implementations significantly degrade\nKANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from\nprohibitive computational costs without proportional performance gains.\nStatistical validation confirms that MLPs with imbalance techniques achieve\nequivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.\nThese findings reveal that KANs represent a specialized solution for raw\nimbalanced data where resources permit. But their severe performance-resource\ntradeoffs and incompatibility with standard resampling techniques currently\nlimits practical deployment. We identify critical research priorities as\ndeveloping KAN specific architectural modifications for imbalance learning,\noptimizing computational efficiency, and theoretical reconciling their conflict\nwith data augmentation. This work establishes foundational insights for next\ngeneration KAN architectures in imbalanced classification scenarios.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T17:50:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14121v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:50:51+00:00"
    },
    {
      "id": "2507.14119v1",
      "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
      "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
      ],
      "abstract": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T17:50:00+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14119v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:50:00+00:00"
    },
    {
      "id": "2507.14116v1",
      "title": "Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification",
      "authors": [
        "Dani\u00eblle Schuman",
        "Mark V. Seebode",
        "Tobias Rohe",
        "Maximilian Balthasar Mansky",
        "Michael Schroedl-Baumann",
        "Jonas Stein",
        "Claudia Linnhoff-Popien",
        "Florian Krellner"
      ],
      "abstract": "Exploiting the fact that samples drawn from a quantum annealer inherently\nfollow a Boltzmann-like distribution, annealing-based Quantum Boltzmann\nMachines (QBMs) have gained increasing popularity in the quantum research\ncommunity. While they harbor great promises for quantum speed-up, their usage\ncurrently stays a costly endeavor, as large amounts of QPU time are required to\ntrain them. This limits their applicability in the NISQ era. Following the idea\nof No\\`e et al. (2024), who tried to alleviate this cost by incorporating\nparallel quantum annealing into their unsupervised training of QBMs, this paper\npresents an improved version of parallel quantum annealing that we employ to\ntrain QBMs in a supervised setting. Saving qubits to encode the inputs, the\nlatter setting allows us to test our approach on medical images from the\nMedMNIST data set (Yang et al., 2023), thereby moving closer to real-world\napplicability of the technology. Our experiments show that QBMs using our\napproach already achieve reasonable results, comparable to those of\nsimilarly-sized Convolutional Neural Networks (CNNs), with markedly smaller\nnumbers of epochs than these classical models. Our parallel annealing technique\nleads to a speed-up of almost 70 % compared to regular annealing-based BM\nexecutions.",
      "categories": [
        "quant-ph",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T17:45:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14116v1",
      "primary_category": "quant-ph",
      "updated": "2025-07-18T17:45:18+00:00"
    },
    {
      "id": "2507.14111v1",
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning",
      "authors": [
        "Xiaoya Li",
        "Xiaofei Sun",
        "Albert Wang",
        "Jiwei Li",
        "Chris Shum"
      ],
      "abstract": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "categories": [
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "published": "2025-07-18T17:43:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14111v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:43:56+00:00"
    },
    {
      "id": "2507.14109v1",
      "title": "An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting",
      "authors": [
        "Xinyu Cao",
        "Bimal Adhikari",
        "Shangqing Zhao",
        "Jingxian Wu",
        "Yanjun Pan"
      ],
      "abstract": "Radio frequency (RF) fingerprinting, which extracts unique hardware\nimperfections of radio devices, has emerged as a promising physical-layer\ndevice identification mechanism in zero trust architectures and beyond 5G\nnetworks. In particular, deep learning (DL) methods have demonstrated\nstate-of-the-art performance in this domain. However, existing approaches have\nprimarily focused on enhancing system robustness against temporal and spatial\nvariations in wireless environments, while the security vulnerabilities of\nthese DL-based approaches have often been overlooked. In this work, we\nsystematically investigate the security risks of DL-based RF fingerprinting\nsystems through an adversarial-driven experimental analysis. We observe a\nconsistent misclassification behavior for DL models under domain shifts, where\na device is frequently misclassified as another specific one. Our analysis\nbased on extensive real-world experiments demonstrates that this behavior can\nbe exploited as an effective backdoor to enable external attackers to intrude\ninto the system. Furthermore, we show that training DL models on raw received\nsignals causes the models to entangle RF fingerprints with environmental and\nsignal-pattern features, creating additional attack vectors that cannot be\nmitigated solely through post-processing security methods such as confidence\nthresholds.",
      "categories": [
        "cs.CR",
        "cs.LG",
        "eess.SP"
      ],
      "published": "2025-07-18T17:42:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14109v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T17:42:20+00:00"
    },
    {
      "id": "2507.14107v1",
      "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment",
      "authors": [
        "Viraj Nishesh Darji",
        "Callie C. Liao",
        "Duoduo Liao"
      ],
      "abstract": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:39:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14107v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:39:03+00:00"
    },
    {
      "id": "2507.14102v1",
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography",
      "authors": [
        "Shravan Venkatraman",
        "Pavan Kumar S",
        "Rakesh Raj Madavan",
        "Chandrakala S"
      ],
      "abstract": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T17:30:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14102v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T17:30:56+00:00"
    },
    {
      "id": "2507.14101v1",
      "title": "Project-connex Decompositions and Tractability of Aggregate Group-by Conjunctive Queries",
      "authors": [
        "Diego Figueira",
        "Cibele Freire"
      ],
      "abstract": "We introduce 'project-connex' tree-width as a measure of tractability for\ncounting and aggregate conjunctive queries over semirings with 'group-by'\nprojection (also known as 'AJAR' or 'FAQ' queries). This elementary measure\nallows to obtain comparable complexity bounds to the ones obtained by previous\nstructural conditions tailored for efficient evaluation of semiring aggregate\nqueries, enumeration algorithms of conjunctive queries, and tractability of\ncounting answers to conjunctive queries.\n  Project-connex tree decompositions are defined as the natural extension of\nthe known notion of 'free-connex' decompositions. They allow for a unified,\nsimple and intuitive algorithmic manipulation for evaluation of aggregate\nqueries and explain some existing tractability results on conjunctive query\nenumeration, counting conjunctive query evaluation, and evaluation of semiring\naggregate queries. Using this measure we also recover results relating\ntractable classes of counting conjunctive queries and bounded free-connex\ntree-width, or the constant-time delay enumeration of semiring aggregate\nqueries over bounded project-connex classes. We further show that\nproject-connex tree decompositions can be obtained via algorithms for computing\nclassical tree decompositions.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T17:30:14+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14101v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T17:30:14+00:00"
    },
    {
      "id": "2507.14097v1",
      "title": "Generative AI-Driven High-Fidelity Human Motion Simulation",
      "authors": [
        "Hari Iyer",
        "Neel Macwan",
        "Atharva Jitendra Hude",
        "Heejin Jeong",
        "Shenghan Guo"
      ],
      "abstract": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T17:24:50+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14097v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T17:24:50+00:00"
    },
    {
      "id": "2507.14096v1",
      "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track",
      "authors": [
        "Brian Ondov",
        "William Xia",
        "Kush Attal",
        "Ishita Unde",
        "Jerry He",
        "Hoa Dang",
        "Ian Soboroff",
        "Dina Demner-Fushman"
      ],
      "abstract": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "published": "2025-07-18T17:23:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14096v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:23:52+00:00"
    },
    {
      "id": "2507.14095v1",
      "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected \u03b4-Overlap Graphs",
      "authors": [
        "Yung-Hong Sun",
        "Ting-Hung Lin",
        "Jiangang Chen",
        "Hongrui Jiang",
        "Yu Hen Hu"
      ],
      "abstract": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:23:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14095v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:23:45+00:00"
    },
    {
      "id": "2507.14093v1",
      "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment",
      "authors": [
        "\u0160imon Kubov",
        "Simon Kl\u00ed\u010dn\u00edk",
        "Jakub Dand\u00e1r",
        "Zden\u011bk Straka",
        "Karol\u00edna Kvakov\u00e1",
        "Daniel Kvak"
      ],
      "abstract": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T17:21:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14093v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:21:53+00:00"
    },
    {
      "id": "2507.14088v1",
      "title": "DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration",
      "authors": [
        "Xiyun Li",
        "Yining Ding",
        "Yuhua Jiang",
        "Yunlong Zhao",
        "Runpeng Xie",
        "Shuang Xu",
        "Yuanhua Ni",
        "Yiqin Yang",
        "Bo Xu"
      ],
      "abstract": "Real-time human-artificial intelligence (AI) collaboration is crucial yet\nchallenging, especially when AI agents must adapt to diverse and unseen human\nbehaviors in dynamic scenarios. Existing large language model (LLM) agents\noften fail to accurately model the complex human mental characteristics such as\ndomain intentions, especially in the absence of direct communication. To\naddress this limitation, we propose a novel dual process multi-scale theory of\nmind (DPMT) framework, drawing inspiration from cognitive science dual process\ntheory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)\nmodule to facilitate robust human partner modeling through mental\ncharacteristic reasoning. Experimental results demonstrate that DPMT\nsignificantly enhances human-AI collaboration, and ablation studies further\nvalidate the contributions of our multi-scale ToM in the slow system.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T17:13:21+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14088v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T17:13:21+00:00"
    },
    {
      "id": "2507.14084v1",
      "title": "The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?",
      "authors": [
        "Maria Tsfasman",
        "Ramin Ghorbani",
        "Catholijn M. Jonker",
        "Bernd Dudzik"
      ],
      "abstract": "Humans have a selective memory, remembering relevant episodes and forgetting\nthe less relevant information. Possessing awareness of event memorability for a\nuser could help intelligent systems in more accurate user modelling, especially\nfor such applications as meeting support systems, memory augmentation, and\nmeeting summarisation. Emotion recognition has been widely studied, since\nemotions are thought to signal moments of high personal relevance to users. The\nemotional experience of situations and their memorability have traditionally\nbeen considered to be closely tied to one another: moments that are experienced\nas highly emotional are considered to also be highly memorable. This\nrelationship suggests that emotional annotations could serve as proxies for\nmemorability. However, existing emotion recognition systems rely heavily on\nthird-party annotations, which may not accurately represent the first-person\nexperience of emotional relevance and memorability. This is why, in this study,\nwe empirically examine the relationship between perceived group emotions\n(Pleasure-Arousal) and group memorability in the context of conversational\ninteractions. Our investigation involves continuous time-based annotations of\nboth emotions and memorability in dynamic, unstructured group settings,\napproximating conditions of real-world conversational AI applications such as\nonline meeting support systems. Our results show that the observed relationship\nbetween affect and memorability annotations cannot be reliably distinguished\nfrom what might be expected under random chance. We discuss the implications of\nthis surprising finding for the development and applications of Affective\nComputing technology. In addition, we contextualise our findings in broader\ndiscourses in the Affective Computing and point out important targets for\nfuture research efforts.",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "published": "2025-07-18T17:06:34+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14084v1",
      "primary_category": "cs.HC",
      "updated": "2025-07-18T17:06:34+00:00"
    },
    {
      "id": "2507.14083v1",
      "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection",
      "authors": [
        "Sara Abdulaziz",
        "Egor Bondarev"
      ],
      "abstract": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T17:06:03+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14083v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T17:06:03+00:00"
    },
    {
      "id": "2507.14079v1",
      "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits",
      "authors": [
        "Garapati Keerthana",
        "Manik Gupta"
      ],
      "abstract": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T17:00:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14079v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T17:00:27+00:00"
    },
    {
      "id": "2507.14077v1",
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions",
      "authors": [
        "Temiloluwa Prioleau",
        "Baiying Lu",
        "Yanjun Cui"
      ],
      "abstract": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T16:53:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14077v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:53:05+00:00"
    },
    {
      "id": "2507.14069v1",
      "title": "Edge Intelligence with Spiking Neural Networks",
      "authors": [
        "Shuiguang Deng",
        "Di Yu",
        "Changze Lv",
        "Xin Du",
        "Linshan Jiang",
        "Xiaofan Zhao",
        "Wentao Tong",
        "Xiaoqing Zheng",
        "Weijia Fang",
        "Peng Zhao",
        "Gang Pan",
        "Schahram Dustdar",
        "Albert Y. Zomaya"
      ],
      "abstract": "The convergence of artificial intelligence and edge computing has spurred\ngrowing interest in enabling intelligent services directly on\nresource-constrained devices. While traditional deep learning models require\nsignificant computational resources and centralized data management, the\nresulting latency, bandwidth consumption, and privacy concerns have exposed\ncritical limitations in cloud-centric paradigms. Brain-inspired computing,\nparticularly Spiking Neural Networks (SNNs), offers a promising alternative by\nemulating biological neuronal dynamics to achieve low-power, event-driven\ncomputation. This survey provides a comprehensive overview of Edge Intelligence\nbased on SNNs (EdgeSNNs), examining their potential to address the challenges\nof on-device learning, inference, and security in edge scenarios. We present a\nsystematic taxonomy of EdgeSNN foundations, encompassing neuron models,\nlearning algorithms, and supporting hardware platforms. Three representative\npractical considerations of EdgeSNN are discussed in depth: on-device inference\nusing lightweight SNN models, resource-aware training and updating under\nnon-stationary data conditions, and secure and privacy-preserving issues.\nFurthermore, we highlight the limitations of evaluating EdgeSNNs on\nconventional hardware and introduce a dual-track benchmarking strategy to\nsupport fair comparisons and hardware-aware optimization. Through this study,\nwe aim to bridge the gap between brain-inspired learning and practical edge\ndeployment, offering insights into current advancements, open challenges, and\nfuture research directions. To the best of our knowledge, this is the first\ndedicated and comprehensive survey on EdgeSNNs, providing an essential\nreference for researchers and practitioners working at the intersection of\nneuromorphic computing and edge intelligence.",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "published": "2025-07-18T16:47:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14069v1",
      "primary_category": "cs.DC",
      "updated": "2025-07-18T16:47:52+00:00"
    },
    {
      "id": "2507.14067v1",
      "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model",
      "authors": [
        "Shuliang Liu",
        "Qi Zheng",
        "Jesse Jiaxi Xu",
        "Yibo Yan",
        "He Geng",
        "Aiwei Liu",
        "Peijie Jiang",
        "Jia Liu",
        "Yik-Cheung Tam",
        "Xuming Hu"
      ],
      "abstract": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T16:44:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14067v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:44:41+00:00"
    },
    {
      "id": "2507.14066v1",
      "title": "Preference-based Multi-Objective Reinforcement Learning",
      "authors": [
        "Ni Mu",
        "Yao Luan",
        "Qing-Shan Jia"
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) is a structured approach for\noptimizing tasks with multiple objectives. However, it often relies on\npre-defined reward functions, which can be hard to design for balancing\nconflicting goals and may lead to oversimplification. Preferences can serve as\nmore flexible and intuitive decision-making guidance, eliminating the need for\ncomplicated reward design. This paper introduces preference-based MORL\n(Pb-MORL), which formalizes the integration of preferences into the MORL\nframework. We theoretically prove that preferences can derive policies across\nthe entire Pareto frontier. To guide policy optimization using preferences, our\nmethod constructs a multi-objective reward model that aligns with the given\npreferences. We further provide theoretical proof to show that optimizing this\nreward model is equivalent to training the Pareto optimal policy. Extensive\nexperiments in benchmark multi-objective tasks, a multi-energy management task,\nand an autonomous driving task on a multi-line highway show that our method\nperforms competitively, surpassing the oracle method, which uses the ground\ntruth reward function. This highlights its potential for practical applications\nin complex real-world systems.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14066v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:43:04+00:00"
    },
    {
      "id": "2507.14063v1",
      "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog",
      "authors": [
        "Lautaro Estienne",
        "Gabriel Ben Zenou",
        "Nona Naderi",
        "Jackie Cheung",
        "Pablo Piantanida"
      ],
      "abstract": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:42:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14063v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:42:22+00:00"
    },
    {
      "id": "2507.14057v1",
      "title": "Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design",
      "authors": [
        "Marcel Hedman",
        "Desi R. Ivanova",
        "Cong Guan",
        "Tom Rainforth"
      ],
      "abstract": "We develop a semi-amortized, policy-based, approach to Bayesian experimental\ndesign (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,\nfully amortized, policy-based BED approaches, Step-DAD trains a design policy\nupfront before the experiment. However, rather than keeping this policy fixed,\nStep-DAD periodically updates it as data is gathered, refining it to the\nparticular experimental instance. This test-time adaptation improves both the\nflexibility and the robustness of the design strategy compared with existing\napproaches. Empirically, Step-DAD consistently demonstrates superior\ndecision-making and robustness compared with current state-of-the-art BED\nmethods.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T16:39:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14057v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T16:39:56+00:00"
    },
    {
      "id": "2507.14056v1",
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in joint training",
      "authors": [
        "Alejandro Rodriguez-Garcia",
        "Anindya Ghosh",
        "Srikanth Ramaswamy"
      ],
      "abstract": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.NC",
        "68T05"
      ],
      "published": "2025-07-18T16:34:06+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14056v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:34:06+00:00"
    },
    {
      "id": "2507.14050v1",
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification",
      "authors": [
        "Mohamed Elkhayat",
        "Mohamed Mahmoud",
        "Jamil Fayyad",
        "Nourhan Bayasi"
      ],
      "abstract": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:15:51+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14050v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:15:51+00:00"
    },
    {
      "id": "2507.14049v1",
      "title": "EdgeVLA: Efficient Vision-Language-Action Models",
      "authors": [
        "Pawe\u0142 Budzianowski",
        "Wesley Maa",
        "Matthew Freed",
        "Jingxiang Mo",
        "Winston Hsiao",
        "Aaron Xie",
        "Tomasz M\u0142oduchowski",
        "Viraj Tipnis",
        "Benjamin Bolte"
      ],
      "abstract": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.",
      "categories": [
        "cs.RO",
        "cs.CL"
      ],
      "published": "2025-07-18T16:15:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14049v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:15:09+00:00"
    },
    {
      "id": "2507.14046v1",
      "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging",
      "authors": [
        "Hao Fang",
        "Hao Yu",
        "Sihao Teng",
        "Tao Zhang",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG"
      ],
      "published": "2025-07-18T16:14:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14046v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T16:14:09+00:00"
    },
    {
      "id": "2507.14045v1",
      "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks",
      "authors": [
        "Israt Jahan",
        "Md Tahmid Rahman Laskar",
        "Chun Peng",
        "Jimmy Huang"
      ],
      "abstract": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T16:13:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14045v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T16:13:35+00:00"
    },
    {
      "id": "2507.14043v1",
      "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems",
      "authors": [
        "Genliang Li",
        "Yaxin Cui",
        "Jinyu Su"
      ],
      "abstract": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CE"
      ],
      "published": "2025-07-18T16:11:35+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14043v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T16:11:35+00:00"
    },
    {
      "id": "2507.14042v1",
      "title": "Training-free Token Reduction for Vision Mamba",
      "authors": [
        "Qiankun Ma",
        "Ziyao Zhang",
        "Chi Su",
        "Jie Chen",
        "Zhen Song",
        "Hairong Zheng",
        "Wen Gao"
      ],
      "abstract": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T16:11:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14042v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T16:11:28+00:00"
    },
    {
      "id": "2507.14038v1",
      "title": "DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis",
      "authors": [
        "Aileen Luo",
        "Tao Zhou",
        "Ming Du",
        "Martin V. Holt",
        "Andrej Singer",
        "Mathew J. Cherukara"
      ],
      "abstract": "Coherent X-ray scattering techniques are critical for investigating the\nfundamental structural properties of materials at the nanoscale. While\nadvancements have made these experiments more accessible, real-time analysis\nremains a significant bottleneck, often hindered by artifacts and computational\ndemands. In scanning X-ray nanodiffraction microscopy, which is widely used to\nspatially resolve structural heterogeneities, this challenge is compounded by\nthe convolution of the divergent beam with the sample's local structure. To\naddress this, we introduce DONUT (Diffraction with Optics for Nanobeam by\nUnsupervised Training), a physics-aware neural network designed for the rapid\nand automated analysis of nanobeam diffraction data. By incorporating a\ndifferentiable geometric diffraction model directly into its architecture,\nDONUT learns to predict crystal lattice strain and orientation in real-time.\nCrucially, this is achieved without reliance on labeled datasets or\npre-training, overcoming a fundamental limitation for supervised machine\nlearning in X-ray science. We demonstrate experimentally that DONUT accurately\nextracts all features within the data over 200 times more efficiently than\nconventional fitting methods.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T16:10:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14038v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T16:10:39+00:00"
    },
    {
      "id": "2507.14032v1",
      "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models",
      "authors": [
        "Lam Nguyen",
        "Erika Barcelos",
        "Roger French",
        "Yinghui Wu"
      ],
      "abstract": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T16:00:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14032v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T16:00:11+00:00"
    },
    {
      "id": "2507.14031v1",
      "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography",
      "authors": [
        "Hao Fang",
        "Sihao Teng",
        "Hao Yu",
        "Siyi Yuan",
        "Huaiwu He",
        "Zhe Liu",
        "Yunjie Yang"
      ],
      "abstract": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.",
      "categories": [
        "cs.CV",
        "cs.ET",
        "cs.LG"
      ],
      "published": "2025-07-18T15:57:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14031v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:57:53+00:00"
    },
    {
      "id": "2507.14024v1",
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "authors": [
        "Jiarong Ye",
        "Sharon X. Huang"
      ],
      "abstract": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:52:39+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14024v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:52:39+00:00"
    },
    {
      "id": "2507.14023v1",
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "authors": [
        "Zhanli Wu",
        "Fabrizio Leisen",
        "F. Javier Rubio"
      ],
      "abstract": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "stat.ME"
      ],
      "published": "2025-07-18T15:51:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14023v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T15:51:48+00:00"
    },
    {
      "id": "2507.14022v1",
      "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis",
      "authors": [
        "Jianfei Li",
        "Kevin Kam Fung Yuen"
      ],
      "abstract": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:41:53+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14022v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:41:53+00:00"
    },
    {
      "id": "2507.14021v1",
      "title": "Byzantine-resilient federated online learning for Gaussian process regression",
      "authors": [
        "Xu Zhang",
        "Zhenyuan Yuan",
        "Minghui Zhu"
      ],
      "abstract": "In this paper, we study Byzantine-resilient federated online learning for\nGaussian process regression (GPR). We develop a Byzantine-resilient federated\nGPR algorithm that allows a cloud and a group of agents to collaboratively\nlearn a latent function and improve the learning performances where some agents\nexhibit Byzantine failures, i.e., arbitrary and potentially adversarial\nbehavior. Each agent-based local GPR sends potentially compromised local\npredictions to the cloud, and the cloud-based aggregated GPR computes a global\nmodel by a Byzantine-resilient product of experts aggregation rule. Then the\ncloud broadcasts the current global model to all the agents. Agent-based fused\nGPR refines local predictions by fusing the received global model with that of\nthe agent-based local GPR. Moreover, we quantify the learning accuracy\nimprovements of the agent-based fused GPR over the agent-based local GPR.\nExperiments on a toy example and two medium-scale real-world datasets are\nconducted to demonstrate the performances of the proposed algorithm.",
      "categories": [
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T15:39:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14021v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:39:47+00:00"
    },
    {
      "id": "2507.14017v1",
      "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models",
      "authors": [
        "Haoyu He",
        "Haozheng Luo",
        "Yan Chen",
        "Qi R. Wang"
      ],
      "abstract": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "published": "2025-07-18T15:31:16+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14017v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T15:31:16+00:00"
    },
    {
      "id": "2507.14013v1",
      "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model",
      "authors": [
        "Ji-Yan Wu",
        "Zheng Yong Poh",
        "Anoop C. Patil",
        "Bongsoo Park",
        "Giovanni Volpe",
        "Daisuke Urano"
      ],
      "abstract": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:25:36+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14013v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:25:36+00:00"
    },
    {
      "id": "2507.14010v1",
      "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations",
      "authors": [
        "Yong Feng",
        "Xiaolei Zhang",
        "Shijin Feng",
        "Yong Zhao",
        "Yihan Chen"
      ],
      "abstract": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T15:21:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14010v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T15:21:02+00:00"
    },
    {
      "id": "2507.14005v1",
      "title": "On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes",
      "authors": [
        "Mathieu Godbout",
        "Audrey Durand"
      ],
      "abstract": "Recent work has shown that dynamic programming (DP) methods for finding\nstatic CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when\nbased on the dual formulation, yet the root cause for the failure has remained\nunclear. We expand on these findings by shifting focus from policy optimization\nto the seemingly simpler task of policy evaluation. We show that evaluating the\nstatic CVaR of a given policy can be framed as two distinct minimization\nproblems. For their solutions to match, a set of ``risk-assignment consistency\nconstraints'' must be satisfied, and we demonstrate that the intersection of\nthe constraints being empty is the source of previously observed evaluation\nerrors. Quantifying the evaluation error as the CVaR evaluation gap, we then\ndemonstrate that the issues observed when optimizing over the dual-based CVaR\nDP are explained by the returned policy having a non-zero CVaR evaluation gap.\nWe then leverage our proposed risk-assignment perspective to prove that the\nsearch for a single, uniformly optimal policy via on the dual CVaR\ndecomposition is fundamentally limited, identifying an MDP where no single\npolicy can be optimal across all initial risk levels.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:18:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14005v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:18:19+00:00"
    },
    {
      "id": "2507.14000v1",
      "title": "Photonic Fabric Platform for AI Accelerators",
      "authors": [
        "Jing Ding",
        "Trung Diep"
      ],
      "abstract": "This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM\n(PFA), a photonic-enabled switch and memory subsystem that delivers low\nlatency, high bandwidth, and low per-bit energy. By integrating high-bandwidth\nHBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D\nelectro-optical system-in-package, the PFA offers up to 32 TB of shared memory\nalongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM\nenables distributed AI training and inference to execute parallelism strategies\nmore efficiently. The Photonic Fabric removes the silicon beachfront constraint\nthat limits the fixed memory-to-compute ratio observed in virtually all current\nXPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet\nthat connects to the Photonic Fabric increases its memory capacity and\ncorrespondingly its memory bandwidth by offering a flexible path to scaling\nwell beyond the limitations of on-package HBM alone. We introduce CelestiSim, a\nlightweight analytical simulator validated on NVIDIA H100 and H200 systems. It\nis used to evaluate the performance of LLM reference and energy savings on PFA,\nwithout any significant change to the GPU core design. With the PFA, the\nsimulation results show that up to 3.66x throughput and 1.40x latency\nimprovements in LLM inference at 405B parameters, up to 7.04x throughput and\n1.41x latency improvements at 1T parameters, and 60-90% energy savings in data\nmovement for heavy collective operations in all LLM training scenarios. While\nthese results are shown for NVIDIA GPUs, they can be applied similarly to other\nAI accelerator designs (XPUs) that share the same fundamental limitation of\nfixed memory to compute.",
      "categories": [
        "cs.PF",
        "cs.AI",
        "C.4"
      ],
      "published": "2025-07-18T15:14:56+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.14000v1",
      "primary_category": "cs.PF",
      "updated": "2025-07-18T15:14:56+00:00"
    },
    {
      "id": "2507.13998v1",
      "title": "ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies",
      "authors": [
        "Itay Katav",
        "Aryeh Kontorovich"
      ],
      "abstract": "Modern multivariate time series forecasting primarily relies on two\narchitectures: the Transformer with attention mechanism and Mamba. In natural\nlanguage processing, an approach has been used that combines local window\nattention for capturing short-term dependencies and Mamba for capturing\nlong-term dependencies, with their outputs averaged to assign equal weight to\nboth. We find that for time-series forecasting tasks, assigning equal weight to\nlong-term and short-term dependencies is not optimal. To mitigate this, we\npropose a dynamic weighting mechanism, ParallelTime Weighter, which calculates\ninterdependent weights for long-term and short-term dependencies for each token\nbased on the input and the model's knowledge. Furthermore, we introduce the\nParallelTime architecture, which incorporates the ParallelTime Weighter\nmechanism to deliver state-of-the-art performance across diverse benchmarks.\nOur architecture demonstrates robustness, achieves lower FLOPs, requires fewer\nparameters, scales effectively to longer prediction horizons, and significantly\noutperforms existing methods. These advances highlight a promising path for\nfuture developments of parallel Attention-Mamba in time series forecasting. The\nimplementation is readily available at:\n\\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T15:08:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13998v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T15:08:02+00:00"
    },
    {
      "id": "2507.13993v1",
      "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models",
      "authors": [
        "Ningyong Wu",
        "Jinzhi Wang",
        "Wenhong Zhao",
        "Chenzhan Yu",
        "Zhigang Xiu",
        "Duwei Dai"
      ],
      "abstract": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "published": "2025-07-18T15:01:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13993v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T15:01:44+00:00"
    },
    {
      "id": "2507.13992v1",
      "title": "Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks",
      "authors": [
        "Jagruti Patel",
        "Thomas A. W. Bolton",
        "Mikkel Sch\u00f6ttner",
        "Anjali Tarun",
        "Sebastien Tourbier",
        "Yasser Alem\u00e0n-G\u00f2mez",
        "Jonas Richiardi",
        "Patric Hagmann"
      ],
      "abstract": "Small sample sizes in neuroimaging in general, and in structural connectome\n(SC) studies in particular limit the development of reliable biomarkers for\nneurological and psychiatric disorders - such as Alzheimer's disease and\nschizophrenia - by reducing statistical power, reliability, and\ngeneralizability. Large-scale multi-site studies have exist, but they have\nacquisition-related biases due to scanner heterogeneity, compromising imaging\nconsistency and downstream analyses. While existing SC harmonization methods -\nsuch as linear regression (LR), ComBat, and deep learning techniques - mitigate\nthese biases, they often rely on detailed metadata, traveling subjects (TS), or\noverlook the graph-topology of SCs. To address these limitations, we propose a\nsite-conditioned deep harmonization framework that harmonizes SCs across\ndiverse acquisition sites without requiring metadata or TS that we test in a\nsimulated scenario based on the Human Connectome Dataset. Within this\nframework, we benchmark three deep architectures - a fully connected\nautoencoder (AE), a convolutional AE, and a graph convolutional AE - against a\ntop-performing LR baseline. While non-graph models excel in edge-weight\nprediction and edge existence detection, the graph AE demonstrates superior\npreservation of topological structure and subject-level individuality, as\nreflected by graph metrics and fingerprinting accuracy, respectively. Although\nthe LR baseline achieves the highest numerical performance by explicitly\nmodeling acquisition parameters, it lacks applicability to real-world\nmulti-site use cases as detailed acquisition metadata is often unavailable. Our\nresults highlight the critical role of model architecture in SC harmonization\nperformance and demonstrate that graph-based approaches are particularly\nwell-suited for structure-aware, domain-generalizable SC harmonization in\nlarge-scale multi-site SC studies.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:58:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13992v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:58:05+00:00"
    },
    {
      "id": "2507.13985v1",
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "authors": [
        "Haoran Li",
        "Yuli Tian",
        "Kun Lan",
        "Yong Liao",
        "Lin Wang",
        "Pan Hui",
        "Peng Yuan Zhou"
      ],
      "abstract": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:45:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13985v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:54+00:00"
    },
    {
      "id": "2507.13984v1",
      "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models",
      "authors": [
        "Quang-Binh Nguyen",
        "Minh Luu",
        "Quang Nguyen",
        "Anh Tran",
        "Khoi Nguyen"
      ],
      "abstract": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T14:45:48+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13984v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:45:48+00:00"
    },
    {
      "id": "2507.13981v1",
      "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset",
      "authors": [
        "Sara Abdulaziz",
        "Giacomo D'Amicantonio",
        "Egor Bondarev"
      ],
      "abstract": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:43:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13981v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:43:24+00:00"
    },
    {
      "id": "2507.13977v1",
      "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic",
      "authors": [
        "Lilit Grigoryan",
        "Nikolay Karpov",
        "Enas Albasiri",
        "Vitaly Lavrukhin",
        "Boris Ginsburg"
      ],
      "abstract": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.",
      "categories": [
        "cs.CL",
        "eess.AS",
        "I.5.1"
      ],
      "published": "2025-07-18T14:42:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13977v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:42:18+00:00"
    },
    {
      "id": "2507.13974v1",
      "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images",
      "authors": [
        "Jiaqi Lv",
        "Yijie Zhu",
        "Carmen Guadalupe Colin Tenorio",
        "Brinder Singh Chohan",
        "Mark Eastwood",
        "Shan E Ahmed Raza"
      ],
      "abstract": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "published": "2025-07-18T14:38:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13974v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T14:38:25+00:00"
    },
    {
      "id": "2507.13970v1",
      "title": "A segmented robot grasping perception neural network for edge AI",
      "authors": [
        "Casper Br\u00f6cheler",
        "Thomas Vroom",
        "Derrick Timmermans",
        "Alan van den Akker",
        "Guangzhi Tang",
        "Charalampos S. Kouzinopoulos",
        "Rico M\u00f6ckel"
      ],
      "abstract": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.",
      "categories": [
        "cs.RO",
        "cs.AI",
        "I.2; I.2.9; I.2.10"
      ],
      "published": "2025-07-18T14:32:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13970v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T14:32:45+00:00"
    },
    {
      "id": "2507.13966v1",
      "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need",
      "authors": [
        "Bhishma Dedhia",
        "Yuval Kansal",
        "Niraj K. Jha"
      ],
      "abstract": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:30:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13966v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:30:08+00:00"
    },
    {
      "id": "2507.13959v1",
      "title": "Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs",
      "authors": [
        "Eli Verwimp",
        "Gustav Ryberg Smidt",
        "Hendrik Hameeuw",
        "Katrien De Graef"
      ],
      "abstract": "The work in this paper describes the training and evaluation of machine\nlearning (ML) techniques for the classification of cuneiform signs. There is a\nlot of variability in cuneiform signs, depending on where they come from, for\nwhat and by whom they were written, but also how they were digitized. This\nvariability makes it unlikely that an ML model trained on one dataset will\nperform successfully on another dataset. This contribution studies how such\ndifferences impact that performance. Based on our results and insights, we aim\nto influence future data acquisition standards and provide a solid foundation\nfor future cuneiform sign classification tasks. The ML model has been trained\nand tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary\ntexts inscribed on clay tablets originating from three Mesopotamian cities\n(Nippur, D\\=ur-Abie\\v{s}uh and Sippar). The presented and analysed model is\nResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for\nsigns with at least 20 instances. As these automatic classification results are\nthe first on Old Babylonian texts, there are currently no comparable results.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:24:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13959v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:24:22+00:00"
    },
    {
      "id": "2507.13958v1",
      "title": "Towards Constraint Temporal Answer Set Programming",
      "authors": [
        "Pedro Cabalar",
        "Mart\u00edn Di\u00e9guez",
        "Fran\u00e7ois Olivier",
        "Torsten Schaub",
        "Igor St\u00e9phan"
      ],
      "abstract": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "published": "2025-07-18T14:22:38+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13958v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:22:38+00:00"
    },
    {
      "id": "2507.13957v1",
      "title": "DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation",
      "authors": [
        "Yitong Li",
        "Raoul Grasman"
      ],
      "abstract": "The modern recommender systems are facing an increasing challenge of\nmodelling and predicting the dynamic and context-rich user preferences.\nTraditional collaborative filtering and content-based methods often struggle to\ncapture the temporal patternings and evolving user intentions. While Large\nLanguage Models (LLMs) have gained gradual attention in recent years, by their\nstrong semantic understanding and reasoning abilities, they are not inherently\ndesigned to model chronologically evolving user preference and intentions. On\nthe other hand, for sequential models like LSTM (Long-Short-Term-Memory) which\nis good at capturing the temporal dynamics of user behaviour and evolving user\npreference over time, but still lacks a rich semantic understanding for\ncomprehensive recommendation generation. In this study, we propose DUALRec\n(Dynamic User-Aware Language-based Recommender), a novel recommender that\nleverages the complementary strength of both models, which combines the\ntemporal modelling abilities of LSTM networks with semantic reasoning power of\nthe fine-tuned Large Language Models. The LSTM component will capture users\nevolving preference through their viewing history, while the fine-tuned LLM\nvariants will leverage these temporal user insights to generate next movies\nthat users might enjoy. Experimental results on MovieLens-1M dataset shows that\nthe DUALRec model outperforms a wide range of baseline models, with\ncomprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted\nCumulative Gain (NDCG@k), and genre similarity metrics. This research proposes\na novel architecture that bridges the gap between temporal sequence modeling\nand semantic reasoning, and offers a promising direction for developing more\nintelligent and context-aware recommenders.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG",
        "68T05, 68T50, 62M45",
        "H.3.3; I.2.6; H.3.4; I.2.7"
      ],
      "published": "2025-07-18T14:22:05+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13957v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T14:22:05+00:00"
    },
    {
      "id": "2507.13956v1",
      "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction",
      "authors": [
        "Yutao Jin",
        "Haowen Xiao",
        "Jielei Chu",
        "Fengmao Lv",
        "Yuxiao Li",
        "Tianrui Li"
      ],
      "abstract": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:21:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13956v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T14:21:24+00:00"
    },
    {
      "id": "2507.13954v1",
      "title": "Robust Anomaly Detection with Graph Neural Networks using Controllability",
      "authors": [
        "Yifan Wei",
        "Anwar Said",
        "Waseem Abbas",
        "Xenofon Koutsoukos"
      ],
      "abstract": "Anomaly detection in complex domains poses significant challenges due to the\nneed for extensive labeled data and the inherently imbalanced nature of\nanomalous versus benign samples. Graph-based machine learning models have\nemerged as a promising solution that combines attribute and relational data to\nuncover intricate patterns. However, the scarcity of anomalous data exacerbates\nthe challenge, which requires innovative strategies to enhance model learning\nwith limited information. In this paper, we hypothesize that the incorporation\nof the influence of the nodes, quantified through average controllability, can\nsignificantly improve the performance of anomaly detection. We propose two\nnovel approaches to integrate average controllability into graph-based\nframeworks: (1) using average controllability as an edge weight and (2)\nencoding it as a one-hot edge attribute vector. Through rigorous evaluation on\nreal-world and synthetic networks with six state-of-the-art baselines, our\nproposed methods demonstrate improved performance in identifying anomalies,\nhighlighting the critical role of controllability measures in enhancing the\nperformance of graph machine learning models. This work underscores the\npotential of integrating average controllability as additional metrics to\naddress the challenges of anomaly detection in sparse and imbalanced datasets.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T14:21:10+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13954v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:21:10+00:00"
    },
    {
      "id": "2507.13950v1",
      "title": "MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space",
      "authors": [
        "Jingbo Liang",
        "Bruna Jacobson"
      ],
      "abstract": "Extensively exploring protein conformational landscapes remains a major\nchallenge in computational biology due to the high computational cost involved\nin dynamic physics-based simulations. In this work, we propose a novel\npipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and\ngenerative adversarial networks (GANs) to explore protein conformational\nspaces. MoDyGAN contains a generator that maps Gaussian distributions into\nMD-derived protein trajectories, and a refinement module that combines ensemble\nlearning with a dual-discriminator to further improve the plausibility of\ngenerated conformations. Central to our approach is an innovative\nrepresentation technique that reversibly transforms 3D protein structures into\n2D matrices, enabling the use of advanced image-based GAN architectures. We use\nthree rigid proteins to demonstrate that MoDyGAN can generate plausible new\nconformations. We also use deca-alanine as a case study to show that\ninterpolations within the latent space closely align with trajectories obtained\nfrom steered molecular dynamics (SMD) simulations. Our results suggest that\nrepresenting proteins as image-like data unlocks new possibilities for applying\nadvanced deep learning techniques to biomolecular simulation, leading to an\nefficient sampling of conformational states. Additionally, the proposed\nframework holds strong potential for extension to other complex 3D structures.",
      "categories": [
        "cs.LG",
        "physics.bio-ph",
        "q-bio.BM"
      ],
      "published": "2025-07-18T14:18:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13950v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T14:18:28+00:00"
    },
    {
      "id": "2507.13949v1",
      "title": "Exploiting Primacy Effect To Improve Large Language Models",
      "authors": [
        "Bianca Raimondi",
        "Maurizio Gabbrielli"
      ],
      "abstract": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T14:18:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13949v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:18:18+00:00"
    },
    {
      "id": "2507.13942v1",
      "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion",
      "authors": [
        "Jacob C Walker",
        "Pedro V\u00e9lez",
        "Luisa Polania Cabrera",
        "Guangyao Zhou",
        "Rishabh Kabra",
        "Carl Doersch",
        "Maks Ovsjanikov",
        "Jo\u00e3o Carreira",
        "Shiry Ginosar"
      ],
      "abstract": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2025-07-18T14:14:19+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13942v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:14:19+00:00"
    },
    {
      "id": "2507.13941v1",
      "title": "Convergent transformations of visual representation in brains and models",
      "authors": [
        "Pablo Marcos-Manch\u00f3n",
        "Llu\u00eds Fuentemilla"
      ],
      "abstract": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CV",
        "eess.IV",
        "I.2.10"
      ],
      "published": "2025-07-18T14:13:54+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13941v1",
      "primary_category": "q-bio.NC",
      "updated": "2025-07-18T14:13:54+00:00"
    },
    {
      "id": "2507.13937v1",
      "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support",
      "authors": [
        "Jan Trienes",
        "Anastasiia Derzhanskaia",
        "Roland Schwarzkopf",
        "Markus M\u00fchling",
        "J\u00f6rg Schl\u00f6tterer",
        "Christin Seifert"
      ],
      "abstract": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T14:09:45+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13937v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T14:09:45+00:00"
    },
    {
      "id": "2507.13934v1",
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "authors": [
        "Marzieh Gheisari",
        "Auguste Genovesio"
      ],
      "abstract": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T14:09:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13934v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:09:18+00:00"
    },
    {
      "id": "2507.13933v1",
      "title": "Preprint: Did I Just Browse A Website Written by LLMs?",
      "authors": [
        "Sichang \"Steven\" He",
        "Ramesh Govindan",
        "Harsha V. Madhyastha"
      ],
      "abstract": "Increasingly, web content is automatically generated by large language models\n(LLMs) with little human input. We call this \"LLM-dominant\" content. Since LLMs\nplagiarize and hallucinate, LLM-dominant content can be unreliable and\nunethical. Yet, websites rarely disclose such content, and human readers\nstruggle to distinguish it. Thus, we must develop reliable detectors for\nLLM-dominant content. However, state-of-the-art LLM detectors are insufficient,\nbecause they perform well mainly on clean, prose-like text, while web content\nhas complex markup and diverse genres.\n  We propose a highly reliable, scalable pipeline that classifies entire\nwebsites. Instead of naively classifying text extracted from each page, we\nclassify each site based on an LLM text detector's outputs of multiple\nprose-like pages. We train and evaluate our detector by collecting 2 distinct\nground truth datasets totaling 120 sites, and obtain 100% accuracies testing\nacross them. In the wild, we detect a sizable portion of sites as LLM-dominant\namong 10k sites in search engine results and 10k in Common Crawl archives. We\nfind LLM-dominant sites are growing in prevalence and rank highly in search\nresults, raising questions about their impact on end users and the overall Web\necosystem.",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "published": "2025-07-18T14:09:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13933v1",
      "primary_category": "cs.NI",
      "updated": "2025-07-18T14:09:04+00:00"
    },
    {
      "id": "2507.13932v1",
      "title": "Chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology",
      "authors": [
        "Feng Yu",
        "Ryan Laird"
      ],
      "abstract": "The rise of blockchain and Digital Ledger Technology (DLT) has gained wide\ntraction. Instead of relying on a traditional centralized data authority, a\nblockchain system consists of digitally entangled block data shared across a\ndistributed network. The specially designed chain data structure and its\nconsensus mechanism protect blockchain data from being tampered by unauthorized\nadversaries. However, implementing a full-fledged blockchain system to protect\na database can be technically cumbersome. In this work, we introduce an\nin-database design, named chain table, to protect data integrity without the\nneed for a blockchain system. It features a succinct design without significant\ntechnology barriers or storage overhead. To realize rigorous data security, we\nalso propose a set of data writing principles for the chain table. We prove\nthat the chain table, together with the data writing principles, will guarantee\nflexible data integrity, named table-level data integrity (TDI).",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "published": "2025-07-18T14:08:24+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13932v1",
      "primary_category": "cs.CR",
      "updated": "2025-07-18T14:08:24+00:00"
    },
    {
      "id": "2507.13929v1",
      "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views",
      "authors": [
        "Hsiang-Hui Hung",
        "Huu-Phu Do",
        "Yung-Hui Li",
        "Ching-Chun Huang"
      ],
      "abstract": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "published": "2025-07-18T14:07:02+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13929v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T14:07:02+00:00"
    },
    {
      "id": "2507.13920v1",
      "title": "Reframing attention as a reinforcement learning problem for causal discovery",
      "authors": [
        "Turan Orujlu",
        "Christian Gumbsch",
        "Martin V. Butz",
        "Charley M Wu"
      ],
      "abstract": "Formal frameworks of causality have operated largely parallel to modern\ntrends in deep reinforcement learning (RL). However, there has been a revival\nof interest in formally grounding the representations learned by neural\nnetworks in causal concepts. Yet, most attempts at neural models of causality\nassume static causal graphs and ignore the dynamic nature of causal\ninteractions. In this work, we introduce Causal Process framework as a novel\ntheory for representing dynamic hypotheses about causal structure. Furthermore,\nwe present Causal Process Model as an implementation of this framework. This\nallows us to reformulate the attention mechanism popularized by Transformer\nnetworks within an RL setting with the goal to infer interpretable causal\nprocesses from visual observations. Here, causal inference corresponds to\nconstructing a causal graph hypothesis which itself becomes an RL task nested\nwithin the original RL problem. To create an instance of such hypothesis, we\nemploy RL agents. These agents establish links between units similar to the\noriginal Transformer attention mechanism. We demonstrate the effectiveness of\nour approach in an RL environment where we outperform current alternatives in\ncausal representation learning and agent performance, and uniquely recover\ngraphs of dynamic causal processes.",
      "categories": [
        "cs.LG"
      ],
      "published": "2025-07-18T13:50:57+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13920v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:50:57+00:00"
    },
    {
      "id": "2507.13919v1",
      "title": "The Levers of Political Persuasion with Conversational AI",
      "authors": [
        "Kobi Hackenburg",
        "Ben M. Tappin",
        "Luke Hewitt",
        "Ed Saunders",
        "Sid Black",
        "Hause Lin",
        "Catherine Fist",
        "Helen Margetts",
        "David G. Rand",
        "Christopher Summerfield"
      ],
      "abstract": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "published": "2025-07-18T13:50:09+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13919v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:50:09+00:00"
    },
    {
      "id": "2507.13915v1",
      "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation",
      "authors": [
        "Huu-Phu Do",
        "Po-Chih Hu",
        "Hao-Chien Hsueh",
        "Che-Kai Liu",
        "Vu-Hoang Tran",
        "Ching-Chun Huang"
      ],
      "abstract": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T13:45:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13915v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:45:04+00:00"
    },
    {
      "id": "2507.13913v1",
      "title": "Political Leaning and Politicalness Classification of Texts",
      "authors": [
        "Matous Volf",
        "Jakub Simko"
      ],
      "abstract": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2025-07-18T13:44:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13913v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T13:44:30+00:00"
    },
    {
      "id": "2507.13912v1",
      "title": "Self-supervised learning on gene expression data",
      "authors": [
        "Kevin Dradjat",
        "Massinissa Hamidi",
        "Pierre Bartet",
        "Blaise Hanczar"
      ],
      "abstract": "Predicting phenotypes from gene expression data is a crucial task in\nbiomedical research, enabling insights into disease mechanisms, drug responses,\nand personalized medicine. Traditional machine learning and deep learning rely\non supervised learning, which requires large quantities of labeled data that\nare costly and time-consuming to obtain in the case of gene expression data.\nSelf-supervised learning has recently emerged as a promising approach to\novercome these limitations by extracting information directly from the\nstructure of unlabeled data. In this study, we investigate the application of\nstate-of-the-art self-supervised learning methods to bulk gene expression data\nfor phenotype prediction. We selected three self-supervised methods, based on\ndifferent approaches, to assess their ability to exploit the inherent structure\nof the data and to generate qualitative representations which can be used for\ndownstream predictive tasks. By using several publicly available gene\nexpression datasets, we demonstrate how the selected methods can effectively\ncapture complex information and improve phenotype prediction accuracy. The\nresults obtained show that self-supervised learning methods can outperform\ntraditional supervised models besides offering significant advantage by\nreducing the dependency on annotated data. We provide a comprehensive analysis\nof the performance of each method by highlighting their strengths and\nlimitations. We also provide recommendations for using these methods depending\non the case under study. Finally, we outline future research directions to\nenhance the application of self-supervised learning in the field of gene\nexpression data analysis. This study is the first work that deals with bulk\nRNA-Seq data and self-supervised learning.",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2025-07-18T13:43:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13912v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T13:43:04+00:00"
    },
    {
      "id": "2507.13901v1",
      "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive",
      "authors": [
        "Lei Xu",
        "Torkel B Brismar"
      ],
      "abstract": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "categories": [
        "eess.IV",
        "cs.CV",
        "62H35, 68U10",
        "I.4.10; I.4.7; J.3"
      ],
      "published": "2025-07-18T13:28:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13901v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T13:28:32+00:00"
    },
    {
      "id": "2507.13899v1",
      "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
      "authors": [
        "Yujian Mo",
        "Yan Wu",
        "Junqiao Zhao",
        "Jijun Wang",
        "Yinghao Hu",
        "Jun Yan"
      ],
      "abstract": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:24:32+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13899v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:24:32+00:00"
    },
    {
      "id": "2507.13892v1",
      "title": "Towards Next Generation Data Engineering Pipelines",
      "authors": [
        "Kevin M. Kramer",
        "Valerie Restat",
        "Sebastian Strasser",
        "Uta St\u00f6rl",
        "Meike Klettke"
      ],
      "abstract": "Data engineering pipelines are a widespread way to provide high-quality data\nfor all kinds of data science applications. However, numerous challenges still\nremain in the composition and operation of such pipelines. Data engineering\npipelines do not always deliver high-quality data. By default, they are also\nnot reactive to changes. When new data is coming in which deviates from prior\ndata, the pipeline could crash or output undesired results. We therefore\nenvision three levels of next generation data engineering pipelines: optimized\ndata pipelines, self-aware data pipelines, and self-adapting data pipelines.\nPipeline optimization addresses the composition of operators and their\nparametrization in order to achieve the highest possible data quality.\nSelf-aware data engineering pipelines enable a continuous monitoring of its\ncurrent state, notifying data engineers on significant changes. Self-adapting\ndata engineering pipelines are then even able to automatically react to those\nchanges. We propose approaches to achieve each of these levels.",
      "categories": [
        "cs.DB"
      ],
      "published": "2025-07-18T13:12:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13892v1",
      "primary_category": "cs.DB",
      "updated": "2025-07-18T13:12:55+00:00"
    },
    {
      "id": "2507.13891v1",
      "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations",
      "authors": [
        "Yu Wei",
        "Jiahui Zhang",
        "Xiaoqin Zhang",
        "Ling Shao",
        "Shijian Lu"
      ],
      "abstract": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T13:09:33+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13891v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T13:09:33+00:00"
    },
    {
      "id": "2507.13887v1",
      "title": "A Survey of Dimension Estimation Methods",
      "authors": [
        "James A. D. Binnie",
        "Pawe\u0142 D\u0142otko",
        "John Harvey",
        "Jakub Malinowski",
        "Ka Man Yim"
      ],
      "abstract": "It is a standard assumption that datasets in high dimension have an internal\nstructure which means that they in fact lie on, or near, subsets of a lower\ndimension. In many instances it is important to understand the real dimension\nof the data, hence the complexity of the dataset at hand. A great variety of\ndimension estimators have been developed to find the intrinsic dimension of the\ndata but there is little guidance on how to reliably use these estimators.\n  This survey reviews a wide range of dimension estimation methods,\ncategorising them by the geometric information they exploit: tangential\nestimators which detect a local affine structure; parametric estimators which\nrely on dimension-dependent probability distributions; and estimators which use\ntopological or metric invariants.\n  The paper evaluates the performance of these methods, as well as\ninvestigating varying responses to curvature and noise. Key issues addressed\ninclude robustness to hyperparameter selection, sample size requirements,\naccuracy in high dimensions, precision, and performance on non-linear\ngeometries. In identifying the best hyperparameters for benchmark datasets,\noverfitting is frequent, indicating that many estimators may not generalise\nwell beyond the datasets on which they have been tested.",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.DG",
        "math.MG",
        "math.ST",
        "stat.TH",
        "62R40 (Primary) 62R30, 62R07, 62G05, 53Z50 (Secondary)"
      ],
      "published": "2025-07-18T13:05:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13887v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T13:05:42+00:00"
    },
    {
      "id": "2507.13881v1",
      "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test",
      "authors": [
        "Cole Walsh",
        "Rodica Ivan",
        "Muhammad Zafar Iqbal",
        "Colleen Robb"
      ],
      "abstract": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "published": "2025-07-18T12:59:17+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13881v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:59:17+00:00"
    },
    {
      "id": "2507.13880v1",
      "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision",
      "authors": [
        "Marten Kreis",
        "Benjamin Kiefer"
      ],
      "abstract": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:58:11+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13880v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:58:11+00:00"
    },
    {
      "id": "2507.13875v1",
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies",
      "authors": [
        "Carlos Mena",
        "Pol Serra",
        "Jacobo Romero",
        "Abir Messaoudi",
        "Jose Giraldo",
        "Carme Armentano-Oller",
        "Rodolfo Zevallos",
        "Ivan Meza",
        "Javier Hernando"
      ],
      "abstract": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "published": "2025-07-18T12:54:41+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13875v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:54:41+00:00"
    },
    {
      "id": "2507.13874v1",
      "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery",
      "authors": [
        "Mateusz Bystro\u0144ski",
        "Miko\u0142aj Ho\u0142ysz",
        "Grzegorz Piotrowski",
        "Nitesh V. Chawla",
        "Tomasz Kajdanowicz"
      ],
      "abstract": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T12:54:28+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13874v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T12:54:28+00:00"
    },
    {
      "id": "2507.13871v1",
      "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models",
      "authors": [
        "Mehul Anand",
        "Shishir Kolathaya"
      ],
      "abstract": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.",
      "categories": [
        "cs.RO",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "published": "2025-07-18T12:50:27+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13871v1",
      "primary_category": "cs.RO",
      "updated": "2025-07-18T12:50:27+00:00"
    },
    {
      "id": "2507.13870v1",
      "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER",
      "authors": [
        "Maciej Jalocha",
        "Johan Hausted Schmidt",
        "William Michelseen"
      ],
      "abstract": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:47:20+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13870v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:47:20+00:00"
    },
    {
      "id": "2507.13868v1",
      "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models",
      "authors": [
        "Francesco Ortu",
        "Zhijing Jin",
        "Diego Doimo",
        "Alberto Cazzaniga"
      ],
      "abstract": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T12:42:30+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13868v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:42:30+00:00"
    },
    {
      "id": "2507.13861v1",
      "title": "PositionIC: Unified Position and Identity Consistency for Image Customization",
      "authors": [
        "Junjie Hu",
        "Tianyang Han",
        "Kai Ma",
        "Jialin Gao",
        "Hao Dou",
        "Song Yang",
        "Xianhua He",
        "Jianhui Zhang",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Wenqiang Zhang"
      ],
      "abstract": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T12:35:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13861v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:35:47+00:00"
    },
    {
      "id": "2507.13859v1",
      "title": "SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection",
      "authors": [
        "Aleksandr Gashkov",
        "Aleksandr Perevalov",
        "Maria Eltsova",
        "Andreas Both"
      ],
      "abstract": "Nowadays, the importance of software with natural-language user interfaces\ncannot be underestimated. In particular, in Question Answering (QA) systems,\ngenerating a SPARQL query for a given natural-language question (often named\nQuery Building) from the information retrieved from the same question is the\ncentral task of QA systems working over Knowledge Graphs (KGQA). Due to the\nrise of Large Language Models (LLMs), they are considered a well-suited method\nto increase the quality of the question-answering functionality, as there is\nstill a lot of room for improvement, aiming for enhanced quality and\ntrustworthiness. However, LLMs are trained on web data, where researchers have\nno control over whether the benchmark or the knowledge graph was already\nincluded in the training data. In this paper, we introduce a novel method that\nevaluates the quality of LLMs by generating a SPARQL query from a\nnatural-language question under various conditions: (1) zero-shot SPARQL\ngeneration, (2) with knowledge injection, and (3) with \"anonymized\" knowledge\ninjection. This enables us, for the first time, to estimate the influence of\nthe training data on the QA quality improved by LLMs. Ultimately, this will\nhelp to identify how portable a method is or whether good results might mostly\nbe achieved because a benchmark was already included in the training data (cf.\nLLM memorization). The developed method is portable, robust, and supports any\nknowledge graph; therefore, it could be easily applied to any KGQA or LLM,\ns.t., generating consistent insights into the actual LLM capabilities is\npossible.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T12:28:08+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13859v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T12:28:08+00:00"
    },
    {
      "id": "2507.13857v1",
      "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
      "authors": [
        "Max van den Hoven",
        "Kishaan Jeeveswaran",
        "Pieter Piscaer",
        "Thijs Wensveen",
        "Elahe Arani",
        "Bahram Zonooz"
      ],
      "abstract": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13857v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13858v1",
      "title": "InTraVisTo: Inside Transformer Visualisation Tool",
      "authors": [
        "Nicol\u00f2 Brunello",
        "Davide Rigamonti",
        "Andrea Sassella",
        "Vincenzo Scotti",
        "Mark James Carman"
      ],
      "abstract": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T12:23:47+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13858v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T12:23:47+00:00"
    },
    {
      "id": "2507.13852v1",
      "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data",
      "authors": [
        "Luigi Russo",
        "Francesco Mauro",
        "Babak Memar",
        "Alessandro Sebastianelli",
        "Silvia Liberata Ullo",
        "Paolo Gamba"
      ],
      "abstract": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "published": "2025-07-18T12:16:04+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13852v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T12:16:04+00:00"
    },
    {
      "id": "2507.13846v1",
      "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments",
      "authors": [
        "Kathrin Korte",
        "Christian Medeiros Adriano",
        "Sona Ghahremani",
        "Holger Giese"
      ],
      "abstract": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:59:55+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13846v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:59:55+00:00"
    },
    {
      "id": "2507.13841v1",
      "title": "Modeling Fair Play in Detective Stories with Language Models",
      "authors": [
        "Eitan Wagner",
        "Renana Keydar",
        "Omri Abend"
      ],
      "abstract": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.",
      "categories": [
        "cs.CL"
      ],
      "published": "2025-07-18T11:55:18+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13841v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:55:18+00:00"
    },
    {
      "id": "2507.13839v1",
      "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words",
      "authors": [
        "Lizhi Ma",
        "Tong Zhao",
        "Shuai Zhang",
        "Nirui Song",
        "Hongliang He",
        "Anqi Li",
        "Ran Feng",
        "Huachuan Qiu",
        "Jingsong Ma",
        "Zhenzhong Lan"
      ],
      "abstract": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "published": "2025-07-18T11:53:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13839v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:53:15+00:00"
    },
    {
      "id": "2507.13835v1",
      "title": "Conformal Data Contamination Tests for Trading or Sharing of Data",
      "authors": [
        "Martin V. Vejling",
        "Shashi Raj Pandey",
        "Christophe A. N. Biscio",
        "Petar Popovski"
      ],
      "abstract": "The amount of quality data in many machine learning tasks is limited to what\nis available locally to data owners. The set of quality data can be expanded\nthrough trading or sharing with external data agents. However, data buyers need\nquality guarantees before purchasing, as external data may be contaminated or\nirrelevant to their specific learning task. Previous works primarily rely on\ndistributional assumptions about data from different agents, relegating quality\nchecks to post-hoc steps involving costly data valuation procedures. We propose\na distribution-free, contamination-aware data-sharing framework that identifies\nexternal data agents whose data is most valuable for model personalization. To\nachieve this, we introduce novel two-sample testing procedures, grounded in\nrigorous theoretical foundations for conformal outlier detection, to determine\nwhether an agent's data exceeds a contamination threshold. The proposed tests,\ntermed conformal data contamination tests, remain valid under arbitrary\ncontamination levels while enabling false discovery rate control via the\nBenjamini-Hochberg procedure. Empirical evaluations across diverse\ncollaborative learning scenarios demonstrate the robustness and effectiveness\nof our approach. Overall, the conformal data contamination test distinguishes\nitself as a generic procedure for aggregating data with statistically rigorous\nquality guarantees.",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "published": "2025-07-18T11:44:42+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13835v1",
      "primary_category": "stat.ML",
      "updated": "2025-07-18T11:44:42+00:00"
    },
    {
      "id": "2507.13834v1",
      "title": "Scalable Submodular Policy Optimization via Pruned Submodularity Graph",
      "authors": [
        "Aditi Anand",
        "Suman Banerjee",
        "Dildar Ali"
      ],
      "abstract": "In Reinforcement Learning (abbreviated as RL), an agent interacts with the\nenvironment via a set of possible actions, and a reward is generated from some\nunknown distribution. The task here is to find an optimal set of actions such\nthat the reward after a certain time step gets maximized. In a traditional\nsetup, the reward function in an RL Problem is considered additive. However, in\nreality, there exist many problems, including path planning, coverage control,\netc., the reward function follows the diminishing return, which can be modeled\nas a submodular function. In this paper, we study a variant of the RL Problem\nwhere the reward function is submodular, and our objective is to find an\noptimal policy such that this reward function gets maximized. We have proposed\na pruned submodularity graph-based approach that provides a provably\napproximate solution in a feasible computation time. The proposed approach has\nbeen analyzed to understand its time and space requirements as well as a\nperformance guarantee. We have experimented with a benchmark agent-environment\nsetup, which has been used for similar previous studies, and the results are\nreported. From the results, we observe that the policy obtained by our proposed\napproach leads to more reward than the baseline methods.",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "published": "2025-07-18T11:42:07+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13834v1",
      "primary_category": "cs.LG",
      "updated": "2025-07-18T11:42:07+00:00"
    },
    {
      "id": "2507.13830v1",
      "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation",
      "authors": [
        "Maximilian Rokuss",
        "Benjamin Hamm",
        "Yannick Kirchhoff",
        "Klaus Maier-Hein"
      ],
      "abstract": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "published": "2025-07-18T11:39:25+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13830v1",
      "primary_category": "eess.IV",
      "updated": "2025-07-18T11:39:25+00:00"
    },
    {
      "id": "2507.13827v1",
      "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models",
      "authors": [
        "Hosein Azarbonyad",
        "Zi Long Zhu",
        "Georgios Cheirmpos",
        "Zubair Afzal",
        "Vikrant Yadav",
        "Georgios Tsatsaronis"
      ],
      "abstract": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.LG"
      ],
      "published": "2025-07-18T11:31:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13827v1",
      "primary_category": "cs.CL",
      "updated": "2025-07-18T11:31:52+00:00"
    },
    {
      "id": "2507.13825v1",
      "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
      "authors": [
        "Haoyang Li",
        "Yuming Xu",
        "Yiming Li",
        "Hanmo Liu",
        "Darian Li",
        "Chen Jason Zhang",
        "Lei Chen",
        "Qing Li"
      ],
      "abstract": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.",
      "categories": [
        "cs.AI"
      ],
      "published": "2025-07-18T11:29:15+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13825v1",
      "primary_category": "cs.AI",
      "updated": "2025-07-18T11:29:15+00:00"
    },
    {
      "id": "2507.13822v1",
      "title": "RAG-based Architectures for Drug Side Effect Retrieval in LLMs",
      "authors": [
        "Shad Nygren",
        "Pinar Avci",
        "Andre Daniels",
        "Reza Rassol",
        "Afshin Beheshti",
        "Diego Galeano"
      ],
      "abstract": "Drug side effects are a major global health concern, necessitating advanced\nmethods for their accurate detection and analysis. While Large Language Models\n(LLMs) offer promising conversational interfaces, their inherent limitations,\nincluding reliance on black-box training data, susceptibility to\nhallucinations, and lack of domain-specific knowledge, hinder their reliability\nin specialized fields like pharmacovigilance. To address this gap, we propose\ntwo architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which\nintegrate comprehensive drug side effect knowledge into a Llama 3 8B language\nmodel. Through extensive evaluations on 19,520 drug side effect associations\n(covering 976 drugs and 3,851 side effect terms), our results demonstrate that\nGraphRAG achieves near-perfect accuracy in drug side effect retrieval. This\nframework offers a highly accurate and scalable solution, signifying a\nsignificant advancement in leveraging LLMs for critical pharmacovigilance\napplications.",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "published": "2025-07-18T11:20:52+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13822v1",
      "primary_category": "cs.IR",
      "updated": "2025-07-18T11:20:52+00:00"
    },
    {
      "id": "2507.13820v1",
      "title": "Team of One: Cracking Complex Video QA with Model Synergy",
      "authors": [
        "Jun Xie",
        "Zhaoran Zhao",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Hongzhu Yi",
        "Xinming Wang",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "abstract": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "published": "2025-07-18T11:12:44+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13820v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T11:12:44+00:00"
    },
    {
      "id": "2507.13812v1",
      "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing",
      "authors": [
        "Yingying Zhang",
        "Lixiang Ru",
        "Kang Wu",
        "Lei Yu",
        "Lei Liang",
        "Yansheng Li",
        "Jingdong Chen"
      ],
      "abstract": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.",
      "categories": [
        "cs.CV"
      ],
      "published": "2025-07-18T10:44:22+00:00",
      "pdf_url": "http://arxiv.org/pdf/2507.13812v1",
      "primary_category": "cs.CV",
      "updated": "2025-07-18T10:44:22+00:00"
    }
  ],
  "metadata": {
    "total_papers": 1000,
    "categories": [
      "cs.IR",
      "cs.CV",
      "cs.PF",
      "cs.DC",
      "cs.RO",
      "cs.CR",
      "cs.HC",
      "cs.NI",
      "stat.ML",
      "cs.DB",
      "eess.IV",
      "q-bio.NC",
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "quant-ph"
    ],
    "fetch_date": "2025-07-21T08:25:47.930961",
    "query_used": "cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.CV OR cat:cs.DB"
  }
}